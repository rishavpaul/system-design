# Order Matching Engine

A production-grade stock exchange order matching engine in Go, built to learn critical system design concepts used in real trading platforms.

**Performance:** Processes **1.5 million orders/second** with **0.7 microsecond** latency on commodity hardware.

> **For Beginners:** Think of this as building the core of Robinhood, E*TRADE, or the NASDAQ. When you click "Buy 100 shares of Apple," this system is what makes that happen by finding someone selling Apple stock and connecting you together.

## Table of Contents

1. [What is an Order Matching Engine?](#what-is-an-order-matching-engine)
2. [Architecture Deep Dive](#architecture-deep-dive)
3. [Core Concepts](#core-concepts)
4. [Data Structures](#data-structures)
5. [Matching Algorithm](#matching-algorithm)
6. [Component Deep Dive](#component-deep-dive)
7. [Running the System](#running-the-system)
8. [Testing & Verification](#testing--verification)
9. [Performance Benchmarks](#performance-benchmarks)
10. [Real-World Comparisons](#real-world-comparisons)
11. [Interview Questions](#interview-questions)
12. [Code Walkthrough](./CODE_WALKTHROUGH.md) â† **Implementation details**

---

## What is an Order Matching Engine?

### The Big Picture

The matching engine is the **heart of any exchange**. It receives buy and sell orders, matches them according to price-time priority, and generates trades.

**Real-world analogy:** Think of it like a smart auction house that runs 24/7:
- **Buyers** submit bids: "I'll pay $150 for Apple stock"
- **Sellers** submit asks: "I'll sell Apple stock for $150"
- The **matching engine** connects them and executes the trade instantly

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Buyers    â”‚â”€â”€â”€â”€ Buy Orders â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”‚   Sellers   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â–¼    â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚    Matching     â”‚
                            â”‚     Engine      â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   Trades    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Matters

Every time you:
- Buy stock on Robinhood
- Trade Bitcoin on Coinbase
- Purchase a call option on TD Ameritrade

...an order matching engine like this one is working behind the scenes.

**Key Responsibilities:**
- **Maintain the order book** (all active buy/sell orders organized by price)
- **Match incoming orders** against resting orders (find buyers for sellers)
- **Execute trades at fair prices** (best price first, then first-come-first-served)
- **Publish market data** (quotes, trades, depth) so traders see real-time prices
- **Ensure deterministic, auditable execution** (same input always produces same output)

### A Concrete Example

```
Initial State:
  Order Book for AAPL (Apple stock)
  SELL orders (asks): $150.25 Ã— 100 shares
  BUY orders (bids):  $150.00 Ã— 100 shares

Trader Alice submits: BUY 100 shares @ $150.50 (willing to pay up to $150.50)

Matching Engine thinks:
  1. Alice wants to buy, so look at SELL orders
  2. Best sell price is $150.25 (cheaper than Alice's $150.50 limit)
  3. Match! Alice buys 100 @ $150.25 (she gets a better price than expected!)

Result:
  - Trade executed: 100 shares @ $150.25
  - Alice paid less than her maximum ($150.50)
  - Seller got their asking price ($150.25)
  - Both parties happy âœ“
```

**This is called "price improvement"** - Alice was willing to pay $150.50 but only paid $150.25 because the matching engine always executes at the best available price.

---

## Architecture Deep Dive

### Understanding the Full System

Before diving into the matching engine itself, let's understand how orders flow through the entire system:

```
Step 1: Client submits order via HTTP
     â†“
Step 2: Gateway validates format (JSON â†’ Order object)
     â†“
Step 3: Risk Checker applies pre-trade controls (is this order safe?)
     â†“
Step 4: Sequencer assigns order number (ensures total ordering)
     â†“
Step 5: Ring Buffer queues event (lock-free, high-performance queue)
     â†“
Step 6: Matching Engine processes order (THE CORE - single-threaded)
     â†“
Step 7: Outcomes split into two paths:
     â”œâ”€â†’ Event Log (records everything for audits/replay)
     â””â”€â†’ Market Data Publisher (broadcasts to traders watching prices)
```

### System Architecture Diagram

```
                           Order Matching Engine Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Client  â”‚â”€â”€â”€â”€â–¶â”‚ Gateway  â”‚â”€â”€â”€â”€â–¶â”‚   Risk   â”‚â”€â”€â”€â”€â–¶â”‚    Sequencer     â”‚  â”‚
â”‚  â”‚ (HTTP)   â”‚     â”‚  (API)   â”‚     â”‚ Checker  â”‚     â”‚  (assigns seq#)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   "Buy AAPL"      Validates        "Is this        "Order #1"â”‚            â”‚
â”‚   100 @ $150      JSON format      safe?"                    â”‚            â”‚
â”‚                                                               â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     SINGLE-THREADED CORE                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ Ring Buffer â”‚â”€â”€â”€â”€â–¶â”‚  Matching   â”‚â”€â”€â”€â”€â–¶â”‚  Order Book (per symbol)â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  (events)   â”‚     â”‚   Engine    â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”     â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚Bids â”‚    â”‚Asks â”‚     â”‚ â”‚ â”‚
â”‚  â”‚   Pre-allocated      Processes           â”‚  â”‚(RBT)â”‚    â”‚(RBT)â”‚     â”‚ â”‚ â”‚
â”‚  â”‚   queue (no locks)   one order           â”‚  â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜     â”‚ â”‚ â”‚
â”‚  â”‚                      at a time            â”‚   Red-Black Trees       â”‚ â”‚ â”‚
â”‚  â”‚                             â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                â”‚                                          â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                    â–¼                       â–¼                              â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚             â”‚  Event Log  â”‚         â”‚  Market     â”‚                       â”‚
â”‚             â”‚  (append)   â”‚         â”‚  Data Pub   â”‚                       â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                    â”‚                       â”‚                              â”‚
â”‚             Writes every              Broadcasts                          â”‚
â”‚             event to disk             to WebSocket                        â”‚
â”‚                    â–¼                       â–¼                              â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚             â”‚  Clearing   â”‚         â”‚  WebSocket  â”‚                       â”‚
â”‚             â”‚   House     â”‚         â”‚  Clients    â”‚                       â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚             Calculates T+2          Traders see                           â”‚
â”‚             settlement              real-time prices                      â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Single-Threaded Core: The LMAX Disruptor Pattern

> **Common Misconception:** "Wait, single-threaded? But I learned that multi-threading makes things faster!"

The matching engine uses a **single-threaded core**, inspired by [LMAX Disruptor](https://lmax-exchange.github.io/disruptor/). This seems counter-intuitive, but it's actually **faster** than multi-threaded approaches for this specific workload.

#### Why Single-Threaded? (The Counter-Intuitive Truth)

Let's address the elephant in the room: Why would we use ONE thread when modern CPUs have 8, 16, or even 64 cores?

**Answer:** Because the work doesn't benefit from parallelism. Here's why:

#### Reason 1: Determinism (Same Input = Same Output, Always)

**Think of it like replaying a video game:**
- In a video game, if you press the same buttons in the same order, you get the same result
- In a multi-threaded system, timing varies - you might get different results each time
- Financial regulators **require** exact reproducibility for audits

```
Single-threaded (deterministic):
  Input: Order #1, Order #2, Order #3
  Output: Trade A, Trade B  â† Always the same

Multi-threaded (non-deterministic):
  Input: Order #1, Order #2, Order #3
  Output: Sometimes Trade A then B, sometimes Trade B then A  â† Race conditions!
```

**Real-world impact:** If the system crashes, we can replay the event log and rebuild the exact same state. With multi-threading, this would be impossible because thread scheduling is non-deterministic.

#### Reason 2: No Locks = Much Faster

**Analogy:** Think of locks like waiting in line for a single bathroom:
- **With locks (multi-threaded):** 10 threads all trying to update the order book, but only 1 can hold the lock at a time. The other 9 wait.
- **Without locks (single-threaded):** No waiting, no contention, just pure work.

```
Cost of locks:
  - Lock acquisition: ~50-100 nanoseconds
  - Context switch: ~1-10 microseconds
  - Cache invalidation: All threads fighting over same data

Cost of single thread:
  - Zero lock overhead
  - Zero context switches
  - All data stays hot in CPU cache
```

**The math:**
- Processing one order: ~100 nanoseconds
- Lock overhead: ~50-100 nanoseconds
- Multi-threading adds 50-100% overhead with no benefit!

#### Reason 3: CPU Cache Magic

**Analogy:** Think of CPU cache like your desk:
- **L1 Cache (your desk):** Instant access, holds ~32 KB
- **L2 Cache (nearby shelf):** Fast access, holds ~256 KB
- **L3 Cache (filing cabinet):** Slower access, holds ~4 MB
- **RAM (warehouse):** Very slow, holds gigabytes

**Single-threaded advantage:**
```
Single thread:
  1. Load order book into L1 cache (32 KB)
  2. Process 1,000 orders (all hitting L1 - super fast!)
  3. Data stays hot in cache (99%+ hit rate)

Multi-threaded:
  1. Thread A loads order book into its core's cache
  2. Thread B modifies order book (Thread A's cache now invalid!)
  3. Threads keep invalidating each other's cache (cache ping-pong)
  4. Cache hit rate drops to ~60-80% (much slower)
```

**Performance impact:**
- L1 cache hit: ~1-2 nanoseconds
- RAM access: ~100 nanoseconds (50-100x slower!)
- Single thread keeps everything in L1 â†’ 50-100x faster

#### Reason 4: The Work is CPU-Bound, Not I/O-Bound

**Critical distinction:**
- **I/O-bound tasks** (web servers, databases): Threads spend most time waiting â†’ multi-threading helps
- **CPU-bound tasks** (matching engine): Pure computation, no waiting â†’ multi-threading adds overhead

```
I/O-bound example (web server):
  Thread 1: [ğŸ” Wait for network...] [âš¡ Process 10ms] [ğŸ” Wait for database...]
  Thread 2:     [âš¡ Process while T1 waits]
  Thread 3:                 [âš¡ Process while T1 and T2 wait]
  â†’ Multi-threading helps! Threads work while others wait.

CPU-bound example (matching engine):
  Thread 1: [âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡ Pure CPU work]
  Thread 2: [ğŸ”’ Blocked on lock...] [âš¡ Brief work] [ğŸ”’ Blocked...]
  Thread 3: [ğŸ”’ Blocked on lock...] [âš¡ Brief work] [ğŸ”’ Blocked...]
  â†’ Multi-threading hurts! Threads mostly wait for locks.
```

**The math:**
- Modern CPU: 3 GHz = 3 billion cycles/second
- Matching one order: ~200-300 cycles
- Theoretical max: 10-15 million orders/second per core
- We get ~1.5 million (with safety checks, logging, etc.)

**Adding more threads doesn't help because:**
1. Orders must be processed in sequence (FIFO requirement)
2. Each order needs to read/modify the same order book
3. Only one thread can safely modify the order book at a time
4. Other threads just wait â†’ wasted CPU cores

#### Visual Comparison

```
Multi-threaded approach (looks good on paper):
   Core 1  Core 2  Core 3  Core 4
    [ğŸ”’]    [âš¡]    [ğŸ”’]    [ğŸ”’]   â† Only 1 doing real work
    [âš¡]    [ğŸ”’]    [ğŸ”’]    [ğŸ”’]   â† 75% of time wasted on locks
    [ğŸ”’]    [ğŸ”’]    [âš¡]    [ğŸ”’]
    [ğŸ”’]    [ğŸ”’]    [ğŸ”’]    [âš¡]

Single-threaded approach (actually faster):
   Core 1  Core 2  Core 3  Core 4
    [âš¡âš¡âš¡]  [ğŸ’¤]    [ğŸ’¤]    [ğŸ’¤]   â† One core always working
    [âš¡âš¡âš¡]  [ğŸ’¤]    [ğŸ’¤]    [ğŸ’¤]   â† No lock overhead
    [âš¡âš¡âš¡]  [ğŸ’¤]    [ğŸ’¤]    [ğŸ’¤]   â† 100% productive time
    [âš¡âš¡âš¡]  [ğŸ’¤]    [ğŸ’¤]    [ğŸ’¤]
```

**But wait, what about those idle cores?**

Great question! We DO use them, just not for matching:
- Core 1: Matching engine (single-threaded core)
- Core 2: Event log writer
- Core 3: Market data publisher
- Core 4: Risk checker
- Core 5-8: API gateway, WebSocket handlers

Each component runs on its own thread, but the **matching core** stays single-threaded.

#### The Key Insight

Matching is **CPU-bound**, not I/O-bound. Parallelism doesn't help when:
1. Work must be done sequentially (FIFO requirement)
2. All threads need the same data (order book)
3. Work is pure computation (no waiting for I/O)

**Parallelism overhead without benefits:**
- Lock contention: 50-100ns per lock
- Cache coherence: MESI protocol overhead
- Context switching: 1-10Âµs per switch
- False sharing: 64-byte cache line invalidations
- Debugging complexity: Race conditions, deadlocks

#### Memory Layout & CPU Cache Optimization

```
CPU Cache Hierarchy:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L1 Cache: 32-64 KB, 1-2 cycles (~0.5ns)   â”‚  â† Order Book lives here
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L2 Cache: 256-512 KB, 4-10 cycles (~2ns)  â”‚  â† Frequently accessed levels
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L3 Cache: 4-32 MB, 40-75 cycles (~15ns)   â”‚  â† Order hash map
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RAM: GBs, 200-300 cycles (~100ns)         â”‚  â† Cold data, event log
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Single-threaded access pattern:
  Sequential reads â†’ Hardware prefetcher predicts next cache line
  No cache invalidations â†’ MESI state always "Exclusive"
  Hot data loops in L1 â†’ 99%+ cache hit rate
```

#### LMAX Achieves 6 Million Orders/Second

Real-world proof:
- **LMAX Exchange**: Retail FX exchange, 6M orders/sec on single thread
- **CME Globex**: Handles 2B messages/day, single-threaded matching
- **Our Implementation**: 1.5M orders/sec (commodity hardware, no tuning)

The pattern works because:
1. **Ring Buffer**: Lock-free, pre-allocated, cache-aligned event queue
2. **Mechanical Sympathy**: Code designed for CPU cache behavior
3. **Zero Garbage**: No allocations in hot path (pre-allocated pools)
4. **Sequential Processing**: CPU branch predictor loves predictable code

---

## Core Concepts

> **Learning Path:** This section explains the fundamental rules that govern how trades happen. If you're new to trading systems, start here before diving into code.

These five concepts form the foundation of any exchange:
1. **Price-Time Priority** - How we decide who gets matched first
2. **Order Types** - Different ways traders can submit orders
3. **Fixed-Point Arithmetic** - Why we never use floating-point for money
4. **Event Sourcing** - How we keep a perfect audit trail
5. **T+2 Settlement** - When money and stocks actually change hands

### 1. Price-Time Priority (FIFO)

**The Fair Matching Rule:** When multiple people want to buy or sell at the same price, who goes first?

**Simple answer:** Best price wins. If prices are equal, first-come-first-served (FIFO).

Orders are matched using **price-time priority**:
1. **Best price first**: Highest bid (buyers), lowest ask (sellers)
2. **First-come-first-served**: At the same price, earlier orders match first

**Why this matters:** This is the law. Exchanges MUST follow price-time priority to ensure fairness. You can't "cut in line" - if someone posted an order before you at the same price, they get filled first.

```
Order Book for AAPL:

ASKS (Sell Orders)              BIDS (Buy Orders)
Price    â”‚ Qty  â”‚ Time          Price    â”‚ Qty  â”‚ Time
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€
$151.00  â”‚ 200  â”‚ 10:01          $150.00  â”‚ 100  â”‚ 09:58  â† Best Bid
$150.50  â”‚ 150  â”‚ 10:00          $149.50  â”‚ 200  â”‚ 09:59
$150.25  â”‚ 100  â”‚ 09:55  â† Best Ask       $149.00  â”‚ 300  â”‚ 10:02

Spread: $0.25 ($150.25 - $150.00)

If a market BUY order for 150 shares arrives:
  â†’ Matches $150.25 Ã— 100 shares (from 09:55 order)
  â†’ Matches $150.50 Ã— 50 shares (partial fill)
  â†’ Average execution price: $150.33
```

#### Price Improvement

**Takers always get maker's price** (or better):

```
Scenario: Aggressive buy order at $151.00 (willing to pay up to $151)
Book has: $150.25 Ã— 100, $150.50 Ã— 50

Result:
  âœ“ Execution at $150.25 (saved $0.75 per share!)
  âœ“ Execution at $150.50 (saved $0.50 per share!)
  âœ— Never executes at $151.00 (taker's limit)

Why? Maker always sets the price. Taker is "price taker."
```

### 2. Order Types

**Beginner Question:** "When I click 'Buy' on Robinhood, what actually happens?"

**Answer:** It depends on the order type you selected! There are 4 common types:

#### Quick Comparison

| Type | What It Means (Plain English) | When to Use | Catch/Risk |
|------|-------------------------------|-------------|------------|
| **Market** | "Buy NOW at whatever price" | You need to own the stock immediately | Might pay more than expected |
| **Limit** | "Buy only if price â‰¤ $X" | You want price protection | Might never execute |
| **IOC** | "Fill what you can NOW, cancel rest" | Okay with partial fills | Might only get 10% filled |
| **FOK** | "Fill entire order NOW or cancel" | All-or-nothing (no partial) | High rejection rate |

**Technical version:**

| Type | Behavior | Time-in-Force | Use Case | Risk |
|------|----------|---------------|----------|------|
| **Market** | Execute immediately at best available price | Immediate | Need execution certainty | Price slippage risk |
| **Limit** | Execute only at specified price or better | Day/GTC | Price protection | May not fill |
| **IOC** | Immediate-or-Cancel: Fill available qty, cancel rest | Immediate | Partial fills acceptable | May get partial fill |
| **FOK** | Fill-or-Kill: Fill entirely or cancel completely | Immediate | All-or-nothing execution | Higher rejection rate |

#### Understanding Through Examples

**Market Order Slippage:**
```
Book depth:
  $150.25 Ã— 100
  $150.50 Ã— 100
  $151.00 Ã— 100

Market BUY 250 shares:
  Fill 1: 100 @ $150.25
  Fill 2: 100 @ $150.50
  Fill 3: 50 @ $151.00
  Avg price: $150.58 (slipped $0.33 from best ask!)
```

**Limit Order Protection:**
```
Limit BUY 250 @ $150.50:
  Fill 1: 100 @ $150.25 âœ“
  Fill 2: 100 @ $150.50 âœ“
  Fill 3: 50 @ $151.00 âœ— (rejected, outside limit)

Result: 200 filled, 50 resting on book at $150.50
```

**IOC vs FOK:**
```
Book: $150.00 Ã— 100

IOC order for 250 @ $150.00:
  â†’ Fills 100, cancels remaining 150
  â†’ Status: Partially filled

FOK order for 250 @ $150.00:
  â†’ Cannot fill entirely
  â†’ Status: Completely rejected (0 filled)
```

### 3. Fixed-Point Arithmetic (No Floats for Money!)

> **Beginner Trap:** "I'll just use `float64` for prices, it's simple!"
>
> **Reality:** This will cause bugs that lose real money and violate financial regulations.

**The Iron Rule:** **Never use floats for money.** Period.

**Why?** Floating-point math has rounding errors that accumulate over millions of trades. This causes:
- Price levels that should match but don't (order book corruption)
- Audit discrepancies (your totals don't add up)
- Regulatory violations (SEC requires exact penny accuracy)
- Lost money (literally! Read about the $300K/day error below)

#### The Problem with Floats (They Lie About Math)

```go
// Floating-point is NOT associative
a := 0.1
b := 0.2
c := 0.3

fmt.Println(a + b == c)  // false!
fmt.Println(a + b)       // 0.30000000000000004

// Accumulating errors
price := 0.0
for i := 0; i < 10; i++ {
    price += 0.1
}
fmt.Println(price == 1.0)  // false!
fmt.Println(price)         // 0.9999999999999999
```

**What's happening?** Binary floating-point can't exactly represent many decimal numbers (like 0.1, 0.2, 0.3). The errors are tiny, but they accumulate.

**Real-world disaster scenario:**
```go
// Order book with float64 prices
orderBook := make(map[float64][]Order)

// Someone posts sell order at 150.25
sellOrder := Order{Price: 150.25, Quantity: 100}
orderBook[150.25] = append(orderBook[150.25], sellOrder)

// Someone posts buy order at 150.25 (should match!)
buyOrder := Order{Price: 150.25, Quantity: 100}

// Look for matching sell orders
sellOrders := orderBook[150.25]  // Should find the sell order, right?

// WRONG! Due to floating-point errors in previous calculations,
// the keys might be 150.24999999999997 vs 150.25000000000003
// Orders don't match! Book is corrupted!
```

#### Fixed-Point Solution (Use Integers!)

**The Solution:** Store everything as **cents** (or smallest currency unit) using **integers**.

```go
// Store prices in cents (int64)
type Price int64  // Cents (never floats!)

const (
    OneCent  Price = 1
    OneDollar Price = 100
)

// $150.25 â†’ 15025 cents (exact integer)
price := Price(15025)

// All operations are exact integer math
total := price * Quantity(100)  // Exact: 1502500 cents = $15,025.00
// No rounding errors, ever!

// Display to user (only use float for display, never storage!)
func (p Price) String() string {
    dollars := p / 100
    cents := p % 100
    return fmt.Sprintf("$%d.%02d", dollars, cents)
}
```

**Why this works:**
- Integers are exact (no rounding errors)
- All math operations are exact
- Can represent any price down to the penny
- Comparisons always work correctly ($150.25 == $150.25, always)

**Performance bonus:** Integer math is faster than floating-point on most CPUs!

#### Real-World Impact (Why This Matters)

**Scenario:** NYSE processes ~3 billion shares/day

```
With float64:
  Tiny error per trade: 0.01Â¢ (one hundredth of a penny)
  3 billion trades Ã— $0.0001 error = $300,000/day in discrepancies!
  Ã— 250 trading days = $75 million/year of "lost" money

With fixed-point integers:
  Error per trade: 0Â¢ (exactly zero)
  3 billion trades Ã— $0.00 error = $0/day
  Perfect accuracy, always
```

**Regulatory requirement:** SEC and all major financial regulators **require exact penny accuracy**. Using floats will fail compliance audits.

### 4. Event Sourcing (Recording History, Not Just State)

> **Traditional Database:** "Bob has $1000 in his account" (current state only)
>
> **Event Sourcing:** "Bob deposited $500 on Jan 1, withdrew $200 on Jan 2, deposited $700 on Jan 3" (complete history)

**The Big Idea:** Instead of storing current state, we store **every state change** (event) that ever happened.

**Analogy:** Think of Git commits vs overwriting a file:
- **Git** (Event Sourcing): Every change is a commit. You can go back to any point in time.
- **Overwriting** (Traditional DB): Only the latest version exists. History is lost.

**In an exchange:** We record every order, every cancel, every fill as a permanent event. The current order book state is just the result of "replaying" all those events.

```
Event Log (Append-Only):
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Seq  â”‚ Timestamp   â”‚ Type   â”‚ Data                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1    â”‚ 09:30:00.0  â”‚ NEW    â”‚ {id:1, symbol:AAPL, buy, $150, 100}  â”‚
â”‚ 2    â”‚ 09:30:00.1  â”‚ NEW    â”‚ {id:2, symbol:AAPL, sell, $150, 50}  â”‚
â”‚ 3    â”‚ 09:30:00.2  â”‚ FILL   â”‚ {trade:1, price:$150, qty:50}        â”‚
â”‚ 4    â”‚ 09:30:01.5  â”‚ CANCEL â”‚ {id:1, remaining:50}                 â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Benefits

| Benefit | Explanation | Regulatory Requirement |
|---------|-------------|------------------------|
| **Audit trail** | Complete history of every action | MiFID II (EU), SEC Rule 613 (US) |
| **Replay** | Rebuild state after crash | Can resume from any point |
| **Time travel** | Query historical state | "What was the book at 09:30:15?" |
| **Debugging** | Reproduce any bug exactly | Replay events until bug manifests |
| **Compliance** | Prove fair execution | Show order precedence was maintained |

#### Event Replay

```go
// Rebuild order book from event log
func Replay(log EventLog) *Engine {
    engine := NewEngine()

    log.ForEach(func(seq uint64, event Event) error {
        switch e := event.(type) {
        case *NewOrderEvent:
            engine.ProcessOrder(e.ToOrder())
        case *CancelEvent:
            engine.CancelOrder(e.OrderID)
        case *FillEvent:
            // Fills are derived from NewOrder events
            // No need to replay separately
        }
        return nil
    })

    return engine
}
```

**Recovery Time:**
- 10M events @ 5M events/sec = 2 seconds recovery
- With snapshots: Start from latest snapshot, replay Î”
- Snapshot every 10M events â†’ Recovery < 2 seconds

### 5. T+2 Settlement

Trades don't settle immediately. In US markets, settlement is **T+2** (trade date + 2 business days).

```
T+0 (Trade Date)     T+1 (Clearing)       T+2 (Settlement)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Order matched        Netting calculated   Securities delivered
Trade reported       Margin verified      Cash exchanged
Both parties         Settlement           Ownership updated
notified             instructions sent
```

#### Why T+2?

**Historical Evolution:**
- **1900s-1960s**: T+5 (physical certificates, mail delivery)
- **1970s-1990s**: T+3 (electronic records)
- **2017**: T+2 (current standard)
- **2024**: T+1 (planned for US markets)

**Why not T+0?**
1. **Financing**: Buyers need time to arrange cash
2. **Securities location**: Sellers need time to locate shares
3. **Operational risk**: Systems need time for reconciliation
4. **Failed trades**: Discover and resolve failures before settlement

#### Netting Example

```
Trader A â†” Trader B in AAPL:

Without netting (3 settlements):
  10:00 - A buys 100 from B @ $150.00
  11:00 - A sells 60 to B @ $151.00
  14:00 - A buys 40 from B @ $149.00

  Settlement requires: 3 Ã— DVP operations
  Securities transferred: 200 shares (100 + 60 + 40)
  Cash transferred: $29,900

With netting (1 settlement):
  Net: A buys 80 from B @ weighted avg

  Settlement requires: 1 Ã— DVP operation
  Securities transferred: 80 shares
  Cash transferred: calculated from weighted average

  Efficiency: 67% reduction in transfers!
```

**Delivery vs Payment (DVP):**
```go
// Atomic exchange - either both happen or neither
func Settle(instruction SettlementInstruction) error {
    return database.Transaction(func(tx *Tx) error {
        // Transfer securities
        tx.Debit(seller.Holdings, symbol, quantity)
        tx.Credit(buyer.Holdings, symbol, quantity)

        // Transfer cash
        tx.Debit(buyer.Cash, cashAmount)
        tx.Credit(seller.Cash, cashAmount)

        return nil  // Commit both or rollback both
    })
}
```

---

## Data Structures

> **Why This Section Matters:** The choice of data structures makes or breaks performance. The wrong choice and you get 100 orders/sec. The right choice and you get 1.5 million orders/sec.

### Understanding What We Need

Before diving into Red-Black Trees and Linked Lists, let's understand the **requirements**:

**What operations must be fast?**
1. **Get best bid/ask** - Every trader wants to see the current best price
2. **Add order** - New orders arrive constantly
3. **Cancel order** - Traders cancel orders all the time
4. **Match order** - Find and execute against resting orders
5. **Get depth** - Show top 5-10 price levels (for market data)

**What are the performance targets?**
- Best bid/ask: Must be **O(1)** - instant lookup
- Add order: Should be **O(log n)** - fast tree insert
- Cancel: Should be **O(1)** - direct removal without searching
- Match: Should be **O(m Ã— log n)** where m = fills (usually < 10)

**The challenge:** We need:
- Fast price lookup (sorted by price) â†’ Tree structure
- FIFO ordering at each price (time priority) â†’ Queue structure
- Fast cancellation by order ID â†’ Hash map lookup

**The solution:** Combine three data structures:
1. **Red-Black Tree** (for price levels, sorted by price)
2. **Doubly-Linked List** (for FIFO queue at each price)
3. **Hash Map** (for O(1) order lookup by ID)

### Order Book Architecture

```
                         OrderBook
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                 â”‚
      Bids (RB-Tree)                   Asks (RB-Tree)
      sorted: highâ†’low                 sorted: lowâ†’high
      descending=true                  descending=false
            â”‚                                 â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
     â–¼      â–¼      â–¼                   â–¼      â–¼      â–¼
  $150   $149.5  $149                $150.25 $150.5 $151
     â”‚                                    â”‚
     â–¼                                    â–¼
  PriceLevel                          PriceLevel
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Price: $150.00      â”‚             â”‚ Price: $150.25      â”‚
  â”‚ TotalQty: 250       â”‚             â”‚ TotalQty: 100       â”‚
  â”‚ Count: 3 orders     â”‚             â”‚ Count: 1 order      â”‚
  â”‚ Orders: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚ Orders: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”˜
                      â–¼                                   â–¼
              Doubly-Linked List                  Doubly-Linked List
              (FIFO order queue)                  (FIFO order queue)

              [Order1]â—„â”€â”€â–º[Order2]â—„â”€â”€â–º[Order3]
               100sh       100sh       50sh
               09:55       09:58       10:01
              (oldest)                (newest)

Plus:
  Order ID Map: hash[OrderID] â†’ *OrderNode
  (For O(1) cancellation by ID)
```

### How It Works: A Concrete Example

Let's walk through adding and matching orders to see how the data structures work together:

**Step 1: Add sell order - 100 shares @ $150.25**
```
1. Check RB-Tree: Does price level $150.25 exist?
   â†’ No, create new PriceLevel node
2. Insert into RB-Tree: Add $150.25 level to Asks tree
   â†’ Tree stays balanced (O(log n) operation)
3. Create OrderNode for this order
4. Append to PriceLevel's linked list (becomes head since it's first)
   â†’ Linked list: [Order1: 100sh @ 09:55]
5. Add to hash map: orderMap[Order1.ID] = OrderNode
   â†’ O(1) lookup for cancellations
```

**Step 2: Add another sell order - 50 shares @ $150.25 (same price!)**
```
1. Check RB-Tree: Does price level $150.25 exist?
   â†’ Yes, found existing PriceLevel
2. Create OrderNode for new order
3. Append to BACK of linked list (FIFO = first in, first out)
   â†’ Linked list: [Order1: 100sh @ 09:55] â†â†’ [Order2: 50sh @ 09:58]
4. Update PriceLevel.TotalQty: 100 + 50 = 150
5. Add to hash map: orderMap[Order2.ID] = OrderNode
```

**Step 3: Buy order arrives - 120 shares @ market price**
```
1. Get best ask: RB-Tree.GetMin() â†’ $150.25 level
   â†’ O(1) because we cache the min pointer
2. Get first order from level: PriceLevel.Head() â†’ Order1
   â†’ O(1) linked list head access
3. Fill 100 shares from Order1 (completely fills it)
   â†’ Remove Order1 from linked list: O(1)
   â†’ Remove Order1 from hash map: O(1)
4. Still need 20 more shares, next order is Order2
5. Fill 20 shares from Order2 (partial fill)
   â†’ Update Order2.FilledQty = 20
   â†’ Order2 stays in list with 30 remaining
   â†’ Update PriceLevel.TotalQty: 150 - 120 = 30

Final state:
  Linked list at $150.25: [Order2: 30sh remaining]
  Total depth at $150.25: 30 shares
```

**Step 4: Cancel Order2**
```
1. Lookup: orderMap[Order2.ID] â†’ OrderNode
   â†’ O(1) hash map lookup
2. Remove from linked list using prev/next pointers
   â†’ O(1) because doubly-linked
3. Remove from hash map
   â†’ O(1)
4. If price level now empty, remove from RB-Tree
   â†’ O(log n)

Result: $150.25 level removed from order book
```

**Key Insight:** By combining three data structures, we get the best of all worlds:
- RB-Tree gives us sorted prices
- Linked List gives us FIFO ordering
- Hash Map gives us fast cancellation

### Why Red-Black Tree + Doubly-Linked List?

#### Red-Black Tree for Price Levels

**Alternative Considered: Skip List**
```
âœ— Skip List (probabilistic):
  - Average O(log n), worst case O(n)
  - Non-deterministic (randomized)
  - Harder to cache (pointer chasing)

âœ“ Red-Black Tree (guaranteed):
  - Worst case O(log n) guaranteed
  - Deterministic (no randomization)
  - Cache-friendly (tree locality)
```

**Alternative Considered: AVL Tree**
```
AVL Tree:
  - More balanced than RB-Tree
  - Faster lookups (height â‰¤ 1.44 log n)
  - Slower inserts/deletes (more rotations)

RB-Tree:
  - Less balanced than AVL
  - Slightly slower lookups (height â‰¤ 2 log n)
  - Faster inserts/deletes (fewer rotations)

Order book is insert-heavy â†’ RB-Tree wins
```

**RB-Tree Properties:**
```
1. Every node is red or black
2. Root is always black
3. Red nodes cannot have red children
4. Every path from root to null has same # of black nodes

Result: Guaranteed balanced
  Height â‰¤ 2 Ã— logâ‚‚(n + 1)

For 1000 price levels:
  Height â‰¤ 2 Ã— logâ‚‚(1001) â‰ˆ 20 comparisons worst case
```

#### Doubly-Linked List for FIFO Queue

**Why not array?**
```
Array:
  âœ“ Cache-friendly (contiguous memory)
  âœ— O(n) removal from middle (for cancels)
  âœ— O(n) insertion in middle
  âœ— Reallocation overhead

Doubly-Linked List:
  âœ“ O(1) removal from anywhere
  âœ“ O(1) insertion at head/tail
  âœ“ No reallocation
  âœ— Pointer overhead (16 bytes per node)
  âœ— Cache misses (pointer chasing)
```

**Memory Layout:**
```go
type OrderNode struct {
    Order *orders.Order  // 8 bytes (pointer)
    prev  *OrderNode     // 8 bytes
    next  *OrderNode     // 8 bytes
    level *PriceLevel    // 8 bytes
}
// Total: 32 bytes overhead per order

Cache line: 64 bytes
â†’ 2 OrderNodes per cache line
â†’ Traversing 100 orders = ~50 cache line reads
â†’ At 1-2 cycles per hit, ~100-200 cycles total
```

### Complexity Analysis

| Operation | Time Complexity | Explanation |
|-----------|-----------------|-------------|
| **Get Best Bid/Ask** | **O(1)** | Cached min/max pointers in RB-Tree |
| **Add Order** | **O(log P)** | RB-Tree insert + O(1) linked list append<br>P = number of price levels (typically < 1000) |
| **Cancel Order** | **O(1)** amortized | Hash map lookup + O(1) linked list removal<br>O(log P) if price level becomes empty |
| **Match Order** | **O(M Ã— log P)** | M fills Ã— (O(1) match + O(log P) level removal)<br>M = number of fills (typically < 10) |
| **Get Depth** | **O(D)** | Traverse D price levels<br>D = depth requested (typically 5-10) |

**Practical Performance:**
```
Typical order book:
  - 100-500 price levels per side
  - 5-20 orders per level
  - Best bid/ask: O(1) = 1-2 CPU cycles
  - Add order: O(log 500) = ~9 comparisons = ~20 CPU cycles
  - Cancel: O(1) = ~5 CPU cycles (hash lookup)
  - Match: O(3 Ã— log 500) = ~27 comparisons = ~50 CPU cycles

Total order processing: ~100-200 CPU cycles
At 3 GHz: ~33-67 nanoseconds per order
```

---

## Matching Algorithm

### High-Level Flow

```go
func (e *Engine) ProcessOrder(order *Order) *ExecutionResult {
    // 1. Validate order
    if err := validate(order); err != nil {
        return reject(err)
    }

    // 2. Assign sequence number (total ordering)
    order.SequenceNum = e.nextSequence()
    order.ID = e.nextOrderID()
    order.Timestamp = now()

    // 3. Attempt to match against resting orders
    fills := e.matchOrder(order, book)

    // 4. Handle remaining quantity based on order type
    if order.RemainingQty() > 0 {
        switch order.Type {
        case OrderTypeLimit:
            book.AddOrder(order)  // Rest in book
            order.Status = PartiallyFilled
        case OrderTypeIOC:
            order.Status = PartiallyFilled  // Cancel remaining
        case OrderTypeFOK:
            // Impossible - FOK checks fillability first
        case OrderTypeMarket:
            // Impossible - market orders sweep all liquidity
        }
    } else {
        order.Status = Filled
    }

    return &ExecutionResult{
        Order: order,
        Fills: fills,
    }
}
```

### Matching Logic (Step-by-Step)

```go
func (e *Engine) matchOrder(order *Order, book *OrderBook) []Fill {
    var fills []Fill

    // Special case: FOK must check fillability first
    if order.Type == OrderTypeFOK {
        if !canFillEntirely(order, book) {
            return fills  // Empty - FOK rejected
        }
    }

    // Determine matching side based on order direction
    var (
        getMatchLevel   func() *PriceLevel
        priceAcceptable func(bookPrice int64) bool
    )

    if order.Side == Buy {
        // Buy order matches against asks (sell orders)
        getMatchLevel = book.GetBestAsk

        priceAcceptable = func(bookPrice int64) bool {
            // Market order: any price acceptable
            if order.Type == Market {
                return true
            }
            // Limit order: ask must be â‰¤ bid
            return bookPrice <= order.Price
        }
    } else {
        // Sell order matches against bids (buy orders)
        getMatchLevel = book.GetBestBid

        priceAcceptable = func(bookPrice int64) bool {
            if order.Type == Market {
                return true
            }
            // Limit order: bid must be â‰¥ ask
            return bookPrice >= order.Price
        }
    }

    // Match against resting orders
    for order.RemainingQty() > 0 {
        level := getMatchLevel()
        if level == nil {
            break  // No more liquidity
        }

        if !priceAcceptable(level.Price) {
            break  // Price doesn't cross
        }

        // Match against orders at this price level (FIFO)
        for node := level.Head(); node != nil && order.RemainingQty() > 0; {
            makerOrder := node.Order

            // Calculate fill quantity
            fillQty := min(order.RemainingQty(), makerOrder.RemainingQty())

            // Create fill at MAKER'S price (price improvement for taker)
            fill := Fill{
                TradeID:      e.nextTradeID(),
                MakerOrderID: makerOrder.ID,
                TakerOrderID: order.ID,
                Price:        level.Price,  // Maker sets price!
                Quantity:     fillQty,
                Timestamp:    now(),
            }
            fills = append(fills, fill)

            // Update quantities
            order.FilledQty += fillQty
            makerOrder.FilledQty += fillQty

            // Save next node (before potential removal)
            nextNode := node.Next()

            // Remove filled maker order from book
            if makerOrder.IsFilled() {
                book.CancelOrder(makerOrder.ID)
            } else {
                level.UpdateQuantity(-fillQty)
            }

            node = nextNode
        }
    }

    return fills
}
```

### Edge Cases & Optimizations

#### 1. Self-Trade Prevention

```go
// Prevent trader from matching with themselves
if order.AccountID == makerOrder.AccountID {
    // Skip this order, continue to next
    node = node.Next()
    continue
}
```

#### 2. Price Crossing Detection

```go
// Orders can cross during matching but not when resting
func wouldCross(newOrder *Order, book *OrderBook) bool {
    if newOrder.Side == Buy {
        bestAsk := book.GetBestAsk()
        return bestAsk != nil && newOrder.Price >= bestAsk.Price
    } else {
        bestBid := book.GetBestBid()
        return bestBid != nil && newOrder.Price <= bestBid.Price
    }
}
```

#### 3. FOK Fillability Check

```go
func canFillEntirely(order *Order, book *OrderBook) bool {
    remaining := order.Quantity

    // Iterate through price levels
    for level := book.GetBestMatchingLevel(order); level != nil; {
        if level.TotalQty >= remaining {
            return true  // Can fill entirely at this level
        }
        remaining -= level.TotalQty
        level = book.GetNextLevel(level)
    }

    return false  // Not enough liquidity
}
```

#### 4. Minimum Quantity Filters

```go
// Real exchanges support minimum fill quantity
type Order struct {
    Quantity    int64
    MinQuantity int64  // Minimum acceptable fill
}

// Reject partial fills below minimum
if fillQty < order.MinQuantity {
    // Cancel order or skip this match
}
```

### Matching Examples

#### Example 1: Limit Order Partial Fill

```
Initial Book:
  ASK: $150.50 Ã— 100, $151.00 Ã— 200
  BID: $150.00 Ã— 150, $149.50 Ã— 200

Incoming: BUY 250 @ $150.75 (limit)

Step 1: Match vs $150.50 (acceptable: 150.50 â‰¤ 150.75)
  â†’ Fill 100 @ $150.50
  â†’ Remaining: 150

Step 2: Match vs $151.00 (not acceptable: 151.00 > 150.75)
  â†’ Stop matching

Result:
  - Filled: 100 @ $150.50
  - Resting: 150 @ $150.75 (new best bid!)

Final Book:
  ASK: $151.00 Ã— 200
  BID: $150.75 Ã— 150 (NEW), $150.00 Ã— 150, $149.50 Ã— 200
```

#### Example 2: Market Order Sweeps Book

```
Initial Book:
  ASK: $150.00 Ã— 50, $150.25 Ã— 100, $150.50 Ã— 150

Incoming: BUY 250 @ MARKET

Step 1: Match vs $150.00
  â†’ Fill 50 @ $150.00
  â†’ Remaining: 200

Step 2: Match vs $150.25
  â†’ Fill 100 @ $150.25
  â†’ Remaining: 100

Step 3: Match vs $150.50
  â†’ Fill 100 @ $150.50
  â†’ Remaining: 0

Result:
  - Fill 1: 50 @ $150.00
  - Fill 2: 100 @ $150.25
  - Fill 3: 100 @ $150.50
  - Average price: $150.30
  - Slippage: $0.30 from best ask
```

#### Example 3: IOC vs FOK

```
Book: $150.00 Ã— 100

IOC order for 250 @ $150.00:
  Step 1: Match vs $150.00
    â†’ Fill 100 @ $150.00
    â†’ Remaining: 150
  Step 2: No more liquidity at $150.00
    â†’ Cancel remaining 150
  Result: Partially filled (100 shares)

FOK order for 250 @ $150.00:
  Pre-check: Can fill 250 entirely?
    â†’ Total available: 100
    â†’ Cannot fill entirely
    â†’ REJECT without executing
  Result: Completely rejected (0 shares)
```

---

## Component Deep Dive

### 1. Risk Checker (`internal/risk/checker.go`)

Pre-trade risk controls run **before** the matching engine to prevent:
- Fat finger errors (accidentally adding extra zeros)
- Flash crashes (orders far from market)
- Position concentration (too much exposure)
- Operational errors (system bugs)

```go
type RiskChecker struct {
    maxOrderSize     int64   // Max shares per order
    maxOrderValue    int64   // Max dollar value (price Ã— qty)
    maxPositionSize  int64   // Max shares held per symbol
    maxDailyVolume   int64   // Max daily traded value
    priceBandPercent float64 // Max deviation from reference price
}

func (r *RiskChecker) Check(order *Order) Result {
    // 1. Order Size Limit (fat finger protection)
    if order.Quantity > r.maxOrderSize {
        return Reject("exceeds max order size")
    }

    // 2. Order Value Limit
    orderValue := order.Price * order.Quantity
    if orderValue > r.maxOrderValue {
        return Reject("exceeds max order value")
    }

    // 3. Price Band (prevent erroneous prices)
    refPrice := r.GetReferencePrice(order.Symbol)
    band := float64(refPrice) * r.priceBandPercent

    if order.Price < refPrice - int64(band) ||
       order.Price > refPrice + int64(band) {
        return Reject("price outside band")
    }

    // 4. Position Limit (prevent concentration)
    currentPos := r.GetPosition(order.Account, order.Symbol)
    projectedPos := currentPos + order.Quantity

    if abs(projectedPos) > r.maxPositionSize {
        return Reject("would exceed position limit")
    }

    // 5. Daily Volume Limit
    dailyVolume := r.GetDailyVolume(order.Account)
    if dailyVolume + orderValue > r.maxDailyVolume {
        return Reject("would exceed daily volume limit")
    }

    return Accept()
}
```

#### Real-World Examples

**Knight Capital - 2012 ($440M loss in 45 minutes)**
```
What happened:
  - Buggy deployment sent unintended orders
  - NO pre-trade risk controls
  - Executed 4 million orders in 45 minutes
  - Lost $440 million before human intervention

Prevention with risk controls:
  âœ“ Daily volume limit: Would stop after $X million
  âœ“ Position limit: Would stop after accumulating Y shares
  âœ“ Order rate limit: 4M orders in 45 min = anomaly
```

**Flash Crash - 2010**
```
What happened:
  - Large sell algorithm
  - Prices dropped 5-6% in minutes
  - Circuit breakers eventually triggered

Prevention with risk controls:
  âœ“ Price bands: Reject orders >10% from market
  âœ“ Order size limits: Break large orders into smaller
  âœ“ Market maker obligations: Required to post quotes
```

### 2. Event Log (`internal/events/log.go`)

Append-only, durable event storage for regulatory compliance and disaster recovery.

```go
// Binary format on disk (optimized for sequential writes)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Seq (8B) â”‚ Type(2B) â”‚ Len (4B) â”‚ Data (var)â”‚ CRC (4B) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

type EventLog struct {
    file       *os.File
    buffer     *bufio.Writer  // 64KB write buffer
    sequenceNum uint64
    lastSync    time.Time
}

func (log *EventLog) Append(event Event) error {
    // 1. Assign sequence number
    seq := atomic.AddUint64(&log.sequenceNum, 1)

    // 2. Serialize event
    data := event.Marshal()

    // 3. Compute checksum
    crc := crc32.ChecksumIEEE(data)

    // 4. Write to buffer
    binary.Write(log.buffer, binary.LittleEndian, seq)
    binary.Write(log.buffer, binary.LittleEndian, event.Type())
    binary.Write(log.buffer, binary.LittleEndian, uint32(len(data)))
    log.buffer.Write(data)
    binary.Write(log.buffer, binary.LittleEndian, crc)

    // 5. Sync every 100ms (batch for performance)
    if time.Since(log.lastSync) > 100*time.Millisecond {
        log.buffer.Flush()
        log.file.Sync()  // fsync to disk
        log.lastSync = time.Now()
    }

    return nil
}
```

#### Event Replay

```go
func (log *EventLog) Replay(handler func(Event) error) error {
    file, _ := os.Open(log.filename)
    defer file.Close()

    reader := bufio.NewReader(file)

    for {
        // Read header
        var seq uint64
        var eventType uint16
        var length uint32

        binary.Read(reader, binary.LittleEndian, &seq)
        binary.Read(reader, binary.LittleEndian, &eventType)
        binary.Read(reader, binary.LittleEndian, &length)

        // Read data
        data := make([]byte, length)
        io.ReadFull(reader, data)

        // Read and verify checksum
        var crc uint32
        binary.Read(reader, binary.LittleEndian, &crc)

        if crc32.ChecksumIEEE(data) != crc {
            return errors.New("corrupted event")
        }

        // Deserialize and handle event
        event := unmarshalEvent(eventType, data)
        if err := handler(event); err != nil {
            return err
        }
    }
}
```

**Performance:**
- Buffered writes: 100K events/sec
- Sequential disk I/O: 500 MB/sec
- Replay speed: 5M events/sec (in-memory processing)
- Recovery time: 10M events = 2 seconds

### 3. Market Data Publisher (`internal/marketdata/publisher.go`)

Real-time data distribution via non-blocking pub/sub.

```go
type Publisher struct {
    l1Subs    map[string][]chan L1Quote      // Symbol â†’ subscribers
    tradeSubs map[string][]chan TradeReport
    bufferSize int  // Channel buffer size
}

// L1 (Top of Book) - Most common
type L1Quote struct {
    Symbol    string
    BidPrice  int64
    BidSize   int64
    AskPrice  int64
    AskSize   int64
    LastPrice int64
    LastSize  int64
    Timestamp int64
}

// L2 (Depth) - For active traders
type L2Depth struct {
    Symbol string
    Bids   []PriceLevel  // Top N levels
    Asks   []PriceLevel
}

// L3 (Full Book) - For market makers
type L3Book struct {
    Symbol string
    Bids   []*Order  // Every individual order
    Asks   []*Order
}
```

#### Non-Blocking Publish Pattern

```go
// CRITICAL: Never block the matching engine
func (p *Publisher) PublishL1(quote L1Quote) {
    for _, ch := range p.l1Subs[quote.Symbol] {
        select {
        case ch <- quote:
            // Successfully sent
        default:
            // Channel full - drop update
            // Slow subscriber doesn't block fast producers
            atomic.AddInt64(&p.droppedUpdates, 1)
        }
    }
}
```

**Design Tradeoffs:**

| Approach | Pro | Con | Best For |
|----------|-----|-----|----------|
| **Blocking send** | Never lose updates | Slow subscriber blocks engine | Critical data |
| **Non-blocking send** | Fast path never blocks | May drop updates | Real-time feeds |
| **Buffered channels** | Absorbs bursts | Memory overhead | Most cases |
| **Ring buffer** | Lock-free, bounded | More complex | HFT |

**Buffer sizing:**
```
Updates per second: 10,000
Subscriber processing: 5,000/sec
Buffer needed: 5,000 (1 second buffer)

If subscriber falls behind >1 second:
  â†’ Drops start occurring
  â†’ Subscriber should reconnect and snapshot
```

### 4. Settlement & Clearing (`internal/settlement/clearing.go`)

Simulates T+2 settlement with netting.

```go
type ClearingHouse struct {
    trades       map[uint64]*Trade
    accounts     map[string]*Account
    instructions []SettlementInstruction
}

// Netting reduces settlement volume
func (ch *ClearingHouse) CalculateNetting() map[string]NetPosition {
    netPositions := make(map[string]map[string]NetPosition)

    for _, trade := range ch.trades {
        // Buyer: receives shares, owes cash
        buyerPos := netPositions[trade.BuyerAccount][trade.Symbol]
        buyerPos.NetQty += trade.Quantity      // Positive = long
        buyerPos.NetValue += trade.Value       // Owes cash

        // Seller: delivers shares, receives cash
        sellerPos := netPositions[trade.SellerAccount][trade.Symbol]
        sellerPos.NetQty -= trade.Quantity     // Negative = short
        sellerPos.NetValue -= trade.Value      // Receives cash
    }

    return netPositions
}

// DVP: Delivery vs Payment (atomic)
func (ch *ClearingHouse) Settle(instruction SettlementInstruction) error {
    // Begin atomic transaction
    tx := ch.db.Begin()
    defer tx.Rollback()

    // Transfer securities
    tx.Debit(seller.Holdings[symbol], quantity)
    tx.Credit(buyer.Holdings[symbol], quantity)

    // Transfer cash
    tx.Debit(buyer.Cash, cashAmount)
    tx.Credit(seller.Cash, cashAmount)

    // Commit atomically
    return tx.Commit()
}
```

**Netting Efficiency:**
```
Example: 1000 trades between same 100 accounts

Without netting:
  1000 settlement instructions
  1000 Ã— DVP operations

With netting:
  ~100-200 settlement instructions (80% reduction)
  Fewer failures (simpler operations)
  Lower operational cost
```

---

## Running the System

### Build and Run

```bash
# Build all packages
cd order-matching-engine
go build ./...

# Run HTTP server
go run ./cmd/server -port 8080

# In another terminal, run demo
go run ./cmd/client demo
```

### API Examples

```bash
# Submit a limit order
curl -X POST localhost:8080/order -d '{
  "symbol": "AAPL",
  "side": "buy",
  "type": "limit",
  "price": "150.00",
  "quantity": 100,
  "account_id": "TRADER1"
}'

# Submit a market order
curl -X POST localhost:8080/order -d '{
  "symbol": "AAPL",
  "side": "sell",
  "type": "market",
  "quantity": 50,
  "account_id": "TRADER2"
}'

# View order book depth
curl "localhost:8080/book?symbol=AAPL&levels=10"

# Cancel order
curl -X DELETE "localhost:8080/cancel?symbol=AAPL&order_id=123"

# Get trade history
curl "localhost:8080/trades?symbol=AAPL&limit=100"
```

---

## Testing & Verification

### Running Tests

```bash
# Run all tests
go test ./tests/... -v

# Run specific test
go test ./tests/... -run TestPriceTimePriority -v

# Run performance benchmark (10M orders)
go test ./tests/... -run TestPerformanceBenchmark -v

# Run correctness verification
go test ./tests/... -run TestCorrectness_VerifyRealMatching -v

# Clear test cache and run fresh
go clean -testcache && go test ./tests/... -v
```

### Test Coverage

The test suite includes **9 comprehensive integration tests**:

| Test | What It Verifies | Why It Matters |
|------|------------------|----------------|
| **TestSingleThreadedCore_Determinism** | Same input â†’ same output (twice) | Critical for replay & audits |
| **TestPriceTimePriority** | FIFO matching at each price level | Core exchange requirement |
| **TestEventSourcing_ReplayCapability** | Can rebuild state from events | Disaster recovery |
| **TestFixedPointArithmetic** | No floating-point errors | Regulatory compliance |
| **TestPreTradeRiskControls** | Risk checks prevent bad orders | Operational safety |
| **TestT2Settlement** | Netting & DVP settlement | Post-trade processing |
| **TestMarketDataPublishing** | Real-time data distribution | Market transparency |
| **TestCorrectness_VerifyRealMatching** | Proves system does real work | Not faking results |
| **TestPerformanceBenchmark** | Throughput measurement | Performance validation |

### Correctness Verification

The `TestCorrectness_VerifyRealMatching` test proves the system is doing real matching:

```
âœ“ Conservation of shares: 425 posted - 225 filled = 200 remaining
âœ“ FIFO enforcement: Orders matched in exact sequence
âœ“ Order book integrity: Levels actually removed after matching
âœ“ Exact quantities: No rounding or approximation errors
âœ“ Price level updates: Best bid/ask changes correctly
```

**How we prove it:**
1. Post 4 sell orders (100, 50, 75, 200 shares)
2. Verify book shows 225 shares at $150.00
3. Buy exactly 225 shares
4. Verify fills match orders in FIFO sequence
5. Verify $150.00 level completely removed
6. Verify remaining 200 shares at $150.50
7. Verify total shares conserved (no magic creation/deletion)

---

## Performance Benchmarks

### Test Environment

- **CPU**: Apple M1 Pro (10 cores, 3.2 GHz)
- **RAM**: 32 GB
- **OS**: macOS
- **Go Version**: 1.21

### Results

```
Processing 10,000,000 orders...

RESULTS:
  Orders processed: 10,000,000
  Time elapsed:     6.75 seconds
  Throughput:       1,482,000 orders/sec
  Latency:          0.67 Âµs/order
  Fills generated:  5,000,000

COMPARISON:
  This engine:  ~1.5M orders/sec
  LMAX:         ~6M orders/sec
  NASDAQ:       ~1M+ msg/sec
```

### Breakdown

| Component | Time per Order | Percentage |
|-----------|----------------|------------|
| Order validation | ~50 ns | 7% |
| Sequence number assignment | ~10 ns | 1.5% |
| Hash map lookup | ~20 ns | 3% |
| RB-Tree traversal | ~50 ns | 7% |
| FIFO queue operations | ~30 ns | 4.5% |
| Fill generation | ~100 ns | 15% |
| Remaining overhead | ~400 ns | 62% |
| **Total** | **~670 ns** | **100%** |

### Optimization Opportunities

**Current implementation (educational focus):**
- General-purpose Go code
- No assembly optimizations
- Standard library data structures
- Safety checks enabled

**Production optimizations would add:**
- Lock-free ring buffer: +2-3x throughput
- Memory pools (zero allocation): +20% throughput
- SIMD for bulk operations: +30% throughput
- Kernel bypass networking: +10x network latency
- NUMA-aware placement: +15% throughput
- Custom memory allocator: +10% throughput

**Realistic production target: 5-10M orders/sec** (matching LMAX)

---

## Real-World Comparisons

### How Real Exchanges Compare

| Aspect | NASDAQ | NYSE | CME | This Implementation |
|--------|--------|------|-----|---------------------|
| **Matching** | Electronic INET | Hybrid (DMM + Electronic) | Electronic Globex | Electronic |
| **Throughput** | 1M+ msg/sec | 1M+ msg/sec | 2B msg/day | 1.5M orders/sec |
| **Latency** | 50-100 Âµs | 50-100 Âµs | 200-500 Âµs | 0.7 Âµs (matching only) |
| **Architecture** | Distributed | Centralized | Distributed | Single-node |
| **Order Types** | 30+ types | 30+ types | 50+ types | 4 types |
| **Symbols** | 3,000+ | 2,800+ | 10,000+ | Unlimited (sharded) |
| **Uptime** | 99.99%+ | 99.99%+ | 99.99%+ | N/A (educational) |

### LMAX Disruptor Pattern

Our design follows [LMAX Exchange](https://www.lmax.com/), a retail FX platform:

```
LMAX Architecture:

  Input â”€â”€â–¶ Sequencer â”€â”€â–¶ Ring Buffer â”€â”€â–¶ Business Logic â”€â”€â–¶ Output
  (HTTP)     (assigns      (lock-free)     (single thread)    (WebSocket)
             seq#)                          MATCHING ENGINE

                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                   â–¼
              Journaler           Replicator
              (disk log)          (network)
```

**LMAX Performance:**
- **6 million transactions/second** on single thread
- **Single-digit microsecond latency** (P99)
- **Zero garbage collection pauses** in critical path
- **100% deterministic replay** from journal

**Key Techniques:**
1. **Mechanical Sympathy**: Code designed for CPU architecture
2. **Ring Buffer**: Pre-allocated, cache-aligned, lock-free queue
3. **Single Writer Principle**: No lock contention
4. **Event Sourcing**: Append-only journal for durability
5. **No Locks in Hot Path**: All coordination via sequencer

---

## Interview Questions

### Conceptual Questions

#### Q: Why use a single-threaded matching engine?

**A:** Three fundamental reasons:

1. **Determinism**: Same input sequence â†’ same output every time
   - Essential for regulatory compliance (prove fair execution)
   - Enables disaster recovery via replay
   - Makes debugging tractable (reproduce any bug exactly)

2. **No locks = No contention**:
   - Lock acquisition: 50-100ns overhead
   - Cache coherence traffic: Additional latency
   - Priority inversion: Low-priority thread holds lock
   - Deadlock risk: Requires complex analysis

3. **CPU cache efficiency**:
   - Single thread keeps order book hot in L1/L2 cache
   - Context switches flush CPU cache (expensive)
   - Sequential access â†’ hardware prefetcher predicts correctly
   - 99%+ cache hit rate (vs ~80% with multi-threading)

**The key insight:** Matching is CPU-bound, not I/O-bound. Parallelism adds overhead without benefits.

---

#### Q: Why not use floats for prices?

**A:** Floating-point arithmetic is fundamentally flawed for money:

```go
// The problem
0.1 + 0.2 == 0.3  // false!
0.1 + 0.2         // 0.30000000000000004

// Accumulating errors
sum := 0.0
for i := 0; i < 10; i++ {
    sum += 0.1
}
sum == 1.0  // false! (sum = 0.9999999999999999)
```

**Real-world consequences:**
- NYSE: 3 billion shares/day Ã— 0.01Â¢ error = **$300,000/day** discrepancy
- Price levels that should match don't â†’ failed trades
- Audit trail discrepancies â†’ regulatory violations
- Accumulating errors â†’ systematic bias

**Solution:** Fixed-point arithmetic (store cents as integers)
- All operations are exact
- No rounding errors
- Deterministic across platforms
- Standard in financial systems

---

#### Q: Why event sourcing instead of a database?

**A:** Event sourcing provides capabilities impossible with traditional databases:

| Capability | Traditional DB | Event Sourcing |
|------------|---------------|----------------|
| **Audit trail** | Only current state | Complete history |
| **Replay** | Cannot rebuild | Replay from any point |
| **Time travel** | Requires versioning | Native support |
| **Debugging** | Lost history | Reproduce exact bug |
| **Performance** | UPDATE queries slow | Append-only (fast) |
| **Compliance** | Additional logging | Built-in |

**Regulatory requirements:**
- **MiFID II (EU)**: Must retain all order/trade data for 5+ years
- **SEC Rule 613 (US)**: Consolidated Audit Trail (CAT) requirement
- **FINRA**: Must prove best execution

**Technical benefits:**
- Append-only writes: 500+ MB/sec sequential I/O
- No UPDATE queries (slow, lock tables)
- Natural disaster recovery (replay events)
- Can derive multiple read models from same events

---

#### Q: How would you scale this to multiple symbols?

**A:** Shard by symbol - each symbol's order book is independent:

```
                    Load Balancer (Hash by Symbol)
                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                   â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Engine 1â”‚         â”‚ Engine 2â”‚    â”‚ Engine 3â”‚
    â”‚         â”‚         â”‚         â”‚    â”‚         â”‚
    â”‚ AAPL    â”‚         â”‚ GOOGL   â”‚    â”‚ MSFT    â”‚
    â”‚ TSLA    â”‚         â”‚ AMZN    â”‚    â”‚ NVDA    â”‚
    â”‚ AMD     â”‚         â”‚ META    â”‚    â”‚ NFLX    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why sharding works:**
- 99% of orders are single-symbol
- No coordination needed between engines
- Horizontal scaling: Add engines for more symbols
- Linear scalability: 3 engines â†’ 3x throughput

**Cross-symbol orders (1% of volume):**
- Pairs trading: AAPL vs SPY
- Spread orders: Buy wheat, sell corn
- Solution: Coordinator service or sequential execution

**Sharding strategy:**
```go
func GetEngine(symbol string) *Engine {
    hash := crc32.ChecksumIEEE([]byte(symbol))
    engineIndex := hash % numEngines
    return engines[engineIndex]
}
```

---

#### Q: How do you handle a node failure?

**A:** Three-layer recovery strategy:

**Layer 1: Event Log Replay**
```go
// New node reads event log and rebuilds state
func Recover(logFile string) *Engine {
    engine := NewEngine()
    log := OpenEventLog(logFile)

    log.Replay(func(event Event) error {
        switch e := event.(type) {
        case *NewOrderEvent:
            engine.ProcessOrder(e.Order)
        case *CancelEvent:
            engine.CancelOrder(e.OrderID)
        }
        return nil
    })

    return engine
}
```

**Layer 2: Real-time Replication**
```
Primary Node                 Secondary Nodes
     â”‚                            â”‚
     â”œâ”€â”€â”€â”€ Event 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ (Apply to local state)
     â”œâ”€â”€â”€â”€ Event 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
     â”œâ”€â”€â”€â”€ Event 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
     â”‚                            â”‚
     X (CRASH)                    â”‚
                                  â”‚
                           (Promote to primary)
```

**Layer 3: Periodic Snapshots**
```
Snapshot every 10M events:
  - Full order book state
  - All sequence numbers
  - Position tracking

Recovery = Load snapshot + Replay Î” events
  10M events @ 5M/sec = 2 seconds recovery
```

**Failover time:**
- Event log only: 10M events Ã· 5M/sec = 2 seconds
- With snapshots: 100K events Ã· 5M/sec = 20ms
- With hot standby: < 1ms (just promote)

---

#### Q: What's the difference between L1, L2, and L3 market data?

**A:** Different levels of detail for different user needs:

| Level | Data | Update Rate | Use Case | Bandwidth |
|-------|------|-------------|----------|-----------|
| **L1** | Best bid/ask only | 1-10/sec | Retail traders | ~1 KB/sec |
| **L2** | Top 5-10 price levels | 10-100/sec | Active traders | ~10 KB/sec |
| **L3** | Every individual order | 100-1000/sec | Market makers | ~100 KB/sec |

**L1 Example (Top of Book):**
```json
{
  "symbol": "AAPL",
  "bidPrice": "150.00",
  "bidSize": 500,
  "askPrice": "150.25",
  "askSize": 300,
  "lastPrice": "150.10",
  "lastSize": 100
}
```

**L2 Example (Depth):**
```json
{
  "symbol": "AAPL",
  "bids": [
    {"price": "150.00", "size": 500},
    {"price": "149.75", "size": 800},
    {"price": "149.50", "size": 1200}
  ],
  "asks": [
    {"price": "150.25", "size": 300},
    {"price": "150.50", "size": 600},
    {"price": "150.75", "size": 400}
  ]
}
```

**L3 Example (Full Book):**
```json
{
  "symbol": "AAPL",
  "orders": [
    {"id": 123, "side": "buy", "price": "150.00", "size": 100},
    {"id": 124, "side": "buy", "price": "150.00", "size": 200},
    {"id": 125, "side": "buy", "price": "150.00", "size": 200},
    // ... every order
  ]
}
```

---

### Coding Questions

#### Q: Implement a price level with O(1) cancel

```go
type PriceLevel struct {
    Price  int64
    orders map[uint64]*OrderNode  // Order ID â†’ Node (O(1) lookup)
    head   *OrderNode             // FIFO queue head
    tail   *OrderNode             // FIFO queue tail
}

type OrderNode struct {
    Order *orders.Order
    prev  *OrderNode
    next  *OrderNode
}

func (pl *PriceLevel) Cancel(orderID uint64) *Order {
    // O(1) hash map lookup
    node := pl.orders[orderID]
    if node == nil {
        return nil
    }

    // O(1) doubly-linked list removal
    if node.prev != nil {
        node.prev.next = node.next
    } else {
        pl.head = node.next
    }

    if node.next != nil {
        node.next.prev = node.prev
    } else {
        pl.tail = node.prev
    }

    delete(pl.orders, orderID)
    return node.Order
}
```

**Key insights:**
- Hash map: O(1) lookup by order ID
- Doubly-linked list: O(1) removal from anywhere
- Trade-off: Extra memory (8 bytes Ã— 2 pointers per order)

---

#### Q: Why might you use a ring buffer for the event queue?

**A:** Ring buffers optimize for the single-producer, single-consumer pattern:

**Properties:**
1. **Lock-free**: No mutexes needed
2. **Cache-friendly**: Contiguous memory, predictable access
3. **Bounded**: Fixed size prevents memory growth
4. **Pre-allocated**: No allocations during operation (no GC pauses)

```go
type RingBuffer struct {
    buffer   []Event
    mask     uint64  // size - 1 for fast modulo
    writePos uint64  // Only producer writes this
    readPos  uint64  // Only consumer writes this
    _padding [64]byte // Prevent false sharing
}

func (rb *RingBuffer) Publish(e Event) bool {
    next := atomic.LoadUint64(&rb.writePos) + 1

    // Check if buffer is full
    if next - atomic.LoadUint64(&rb.readPos) > uint64(len(rb.buffer)) {
        return false  // Full - producer must wait
    }

    // Write to buffer (no lock needed - single writer)
    rb.buffer[rb.writePos & rb.mask] = e

    // Advance write position
    atomic.StoreUint64(&rb.writePos, next)
    return true
}

func (rb *RingBuffer) Consume() (Event, bool) {
    readPos := atomic.LoadUint64(&rb.readPos)
    writePos := atomic.LoadUint64(&rb.writePos)

    // Check if buffer is empty
    if readPos == writePos {
        return Event{}, false
    }

    // Read from buffer (no lock needed - single reader)
    event := rb.buffer[readPos & rb.mask]

    // Advance read position
    atomic.StoreUint64(&rb.readPos, readPos + 1)
    return event, true
}
```

**Performance benefits:**
- No locks: ~50-100ns saved per operation
- Cache-friendly: Sequential access pattern
- Predictable: Hardware prefetcher works well
- LMAX: 100M+ events/sec with ring buffer

---

## Project Structure

```
order-matching-engine/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ server/main.go      # HTTP server, ties components together
â”‚   â””â”€â”€ client/main.go      # CLI client for testing
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ orderbook/          # Order book data structure
â”‚   â”‚   â”œâ”€â”€ orderbook.go    # Main order book logic
â”‚   â”‚   â”œâ”€â”€ pricelevel.go   # Price level with FIFO queue
â”‚   â”‚   â””â”€â”€ rbtree.go       # Red-black tree implementation
â”‚   â”œâ”€â”€ matching/
â”‚   â”‚   â””â”€â”€ engine.go       # Matching engine (single-threaded core)
â”‚   â”œâ”€â”€ orders/
â”‚   â”‚   â””â”€â”€ types.go        # Order, Fill, Trade types
â”‚   â”œâ”€â”€ events/
â”‚   â”‚   â”œâ”€â”€ types.go        # Event type definitions
â”‚   â”‚   â””â”€â”€ log.go          # Append-only event log
â”‚   â”œâ”€â”€ risk/
â”‚   â”‚   â””â”€â”€ checker.go      # Pre-trade risk checks
â”‚   â”œâ”€â”€ settlement/
â”‚   â”‚   â””â”€â”€ clearing.go     # T+2 settlement simulation
â”‚   â””â”€â”€ marketdata/
â”‚       â””â”€â”€ publisher.go    # L1/L2 market data publishing
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ integration_test.go # Comprehensive test suite
â””â”€â”€ api/                    # API definitions (gRPC, WebSocket)
```

---

## Code Walkthrough

> **Want to understand the implementation?** See [CODE_WALKTHROUGH.md](./CODE_WALKTHROUGH.md) for:
> - Component interaction flow with execution traces
> - Critical code patterns explained
> - Data structure deep dive with memory layouts
> - Step-by-step examples of order matching and cancellation

---

## Connecting the Dots: How Everything Works Together

> **For developers who are still unclear:** This section ties all the concepts together to show you the complete picture.

### The Complete Journey of an Order

Let's follow a single order from submission to execution and see how ALL the pieces interact:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Step 1: Order Submission                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Alice clicks "Buy 100 AAPL @ $150.50" on her trading app
  â†“
HTTP POST to /order API endpoint
  â†“
Gateway converts JSON to Order struct
  Order {
    Symbol: "AAPL"
    Side: Buy
    Type: Limit
    Price: 15050  â† Fixed-point (cents!)
    Quantity: 100
    AccountID: "alice_123"
  }

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Step 2: Risk Checking (Pre-Trade Controls)                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RiskChecker.Check(order):
  âœ“ Order size: 100 < 1000 max
  âœ“ Order value: $15,050 < $50,000 max
  âœ“ Price band: $150.50 within 10% of $150 reference
  âœ“ Position limit: Alice won't exceed max position
  â†’ PASS

If any check failed, order would be rejected HERE (never reaches matching)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Step 3: Sequencing (Ensures Total Ordering)                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Sequencer assigns:
  order.SequenceNum = 12345678  â† Global order across ALL symbols
  order.OrderID = 98765         â† Unique order ID
  order.Timestamp = 1640995200  â† Unix timestamp

Why? Ensures deterministic replay - events can be replayed in exact order

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Step 4: Ring Buffer Queuing (Lock-Free Queue)                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ring buffer holds events in memory:
  [Event12345676] [Event12345677] [Event12345678: Alice's order] [...]
                                   â†‘
                                   Read pointer moves here

Single-threaded matching engine reads one event at a time

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Step 5: Matching Engine (THE CORE - Single-Threaded)                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Engine.ProcessOrder(order):

  1. Get AAPL order book from map[symbol] â†’ OrderBook

  2. Alice wants to BUY, so look at SELL orders (asks)

     Order Book State:
       Asks (sellers):
         $150.25: [Order1: 50sh] â† Best ask (lowest price)
         $150.50: [Order2: 100sh]
         $151.00: [Order3: 200sh]

  3. Alice's limit is $150.50, best ask is $150.25
     â†’ Prices cross! Can match!

  4. Match against $150.25 level:
     - RB-Tree.GetBestAsk() â†’ $150.25 PriceLevel
     - PriceLevel.Head() â†’ Order1 (FIFO - first in queue)
     - Fill 50 shares @ $150.25
     - Remove Order1 from linked list (O(1))
     - Remove Order1 from hash map (O(1))

  5. Alice still needs 50 more shares, next level is $150.50
     - Match 50 shares @ $150.50
     - Order2 now has 50 shares remaining
     - Update PriceLevel.TotalQty

  6. Alice's order completely filled!

     Result:
       Fill #1: 50 shares @ $150.25
       Fill #2: 50 shares @ $150.50
       Average price: $150.375

       Alice SAVED money! She was willing to pay $150.50 for all,
       but got 50 shares at $150.25 (price improvement!)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Step 6: Event Logging (Audit Trail)                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EventLog.Append():
  Event #12345678:
    {
      seq: 12345678,
      type: NEW_ORDER,
      data: [Alice's order details]
    }

  Event #12345679:
    {
      seq: 12345679,
      type: FILL,
      data: {
        makerOrderID: Order1.ID,
        takerOrderID: Alice's order ID,
        price: 15025,  â† $150.25 in cents
        quantity: 50
      }
    }

  Event #12345680:
    {
      seq: 12345680,
      type: FILL,
      data: [second fill at $150.50]
    }

Written to disk in append-only log
â†’ If system crashes, replay events to rebuild exact state

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Step 7: Market Data Publishing (Real-Time Broadcast)                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MarketDataPublisher broadcasts to all subscribers:

L1 Update (Top of Book):
  {
    symbol: "AAPL",
    bestBid: 150.00,  â† Highest buy order
    bestAsk: 150.50,  â† Lowest sell order (changed!)
    lastPrice: 150.375,
    lastSize: 100
  }

All traders watching AAPL see updated prices in real-time
  â†’ Robinhood app shows: "AAPL $150.375"
  â†’ Bloomberg terminal updates
  â†’ High-frequency traders' algorithms react

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Step 8: Settlement (T+2 - Two Days Later)                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ClearingHouse calculates settlement on T+2:

Alice's net position today:
  Bought 100 AAPL
  Average price: $150.375
  Total value: $15,037.50

Settlement instruction (T+2):
  Transfer FROM Alice's broker TO seller's broker:
    Cash: $15,037.50

  Transfer FROM seller's broker TO Alice's broker:
    Securities: 100 shares of AAPL

DVP (Delivery vs Payment) - atomic transaction
  â†’ Either both happen or neither (prevents fraud)

After settlement, Alice officially owns 100 AAPL shares
```

### Why Each Component Matters

| Component | Purpose | What Happens If It Breaks |
|-----------|---------|---------------------------|
| **Gateway** | Validates input format | Malformed data crashes matching engine |
| **Risk Checker** | Prevents bad orders | Fat-finger errors, flash crashes (Knight Capital) |
| **Sequencer** | Orders events globally | Non-deterministic replay, audit failures |
| **Ring Buffer** | Lock-free queue | Lock contention slows down matching |
| **Matching Engine** | Core business logic | No trades happen! |
| **Order Book (RB-Tree + List)** | Fast price/time priority | Slow matching, wrong FIFO order |
| **Fixed-Point Arithmetic** | Exact money math | Rounding errors, audit failures, lost money |
| **Event Log** | Audit trail | Can't recover from crash, compliance violations |
| **Market Data** | Real-time prices | Traders can't see current market |
| **Settlement** | Money/shares transfer | Trades execute but never settle |

### Key Design Decisions Recap

1. **Single-Threaded Matching Core**
   - **Why:** Deterministic, cache-efficient, no locks
   - **Trade-off:** Can't use all CPU cores for matching (but we don't need to!)

2. **Red-Black Tree for Price Levels**
   - **Why:** Guaranteed O(log n), deterministic, cache-friendly
   - **Alternative:** AVL tree (more balanced) or Skip List (probabilistic)

3. **Doubly-Linked List for FIFO**
   - **Why:** O(1) insertion/removal, perfect for FIFO
   - **Trade-off:** Memory overhead (pointers), cache misses

4. **Fixed-Point Arithmetic (Integers for Money)**
   - **Why:** Exact math, no rounding errors
   - **Alternative:** None - floats are never acceptable

5. **Event Sourcing**
   - **Why:** Perfect audit trail, replay capability
   - **Trade-off:** More disk space (but disk is cheap)

### Common Misconceptions Addressed

| Misconception | Reality |
|---------------|---------|
| "Multi-threading is always faster" | Not for CPU-bound sequential work |
| "Floats are fine for money if I round carefully" | Rounding errors accumulate; use integers |
| "Just store current state in database" | Need full event history for compliance |
| "Hash map is always O(1)" | Average O(1), worst case O(n) with collisions |
| "AVL tree is better than RB-tree" | Better for reads, worse for writes (we write a lot!) |
| "We need a distributed system to scale" | Shard by symbol - each symbol is independent |

### What You've Learned

If you understand this project, you now understand:

âœ“ **System Design Patterns:**
- Single-threaded core (LMAX Disruptor)
- Event Sourcing
- CQRS (Command Query Responsibility Segregation)
- Pub/Sub for market data

âœ“ **Data Structures:**
- Red-Black Trees (self-balancing BST)
- Doubly-Linked Lists (O(1) removal)
- Hash Maps (O(1) lookup)
- Hybrid structures (combining multiple DS)

âœ“ **Performance Optimization:**
- CPU cache optimization
- Lock-free data structures
- Fixed-point arithmetic
- Zero-allocation hot paths

âœ“ **Financial Concepts:**
- Order matching algorithms
- Price-time priority (FIFO)
- Order types (Market, Limit, IOC, FOK)
- T+2 settlement
- Pre-trade risk controls

âœ“ **Real-World Engineering:**
- Regulatory compliance (SEC, MiFID II)
- Disaster recovery (event replay)
- Deterministic systems
- High-performance systems design

### Next Steps for Learning

1. **Implement it yourself:** Clone this repo and modify it
   - Add a new order type (Stop-Loss)
   - Implement auction matching (opening/closing auctions)
   - Add market maker protections

2. **Read the code:** Start with:
   - `internal/matching/engine.go` - Core matching logic
   - `internal/orderbook/orderbook.go` - Data structure
   - `tests/integration_test.go` - See how it all works

3. **Profile and optimize:**
   - Use `pprof` to find hot spots
   - Try different data structures
   - Measure impact on throughput

4. **Study real exchanges:**
   - LMAX Disruptor (retail FX)
   - NASDAQ INET (equity matching)
   - CME Globex (futures matching)

---

## References

- [LMAX Disruptor](https://lmax-exchange.github.io/disruptor/) - High-performance inter-thread messaging
- [How LMAX Revolutionized Trading](https://martinfowler.com/articles/lmax.html) - Martin Fowler's deep dive
- [NYSE Pillar](https://www.nyse.com/pillar) - NYSE's trading platform architecture
- [NASDAQ TotalView](https://www.nasdaq.com/solutions/nasdaq-totalview) - Full depth market data
- [SEC Rule 613 (CAT)](https://www.sec.gov/rules/final/2012/34-67457.pdf) - Consolidated audit trail requirements
- [MiFID II](https://www.esma.europa.eu/policy-rules/mifid-ii-and-mifir) - EU markets regulation
- [Red-Black Trees](https://en.wikipedia.org/wiki/Red%E2%80%93black_tree) - Self-balancing BST
- [Mechanical Sympathy](https://mechanical-sympathy.blogspot.com/) - Hardware-aware programming
